import akshare as ak
import pandas as pd
import time
import functools
import datetime
import requests
import re
import os
import pickle

CACHE_DIR = "data_cache"
if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

def with_cache(ttl_hours=8):
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # Generate cache key
            arg_str = "_".join([str(a) for a in args])
            kwarg_str = "_".join([f"{k}-{v}" for k, v in kwargs.items()])
            identifier = f"{func.__name__}_{arg_str}_{kwarg_str}"
            # Clean filename
            identifier = "".join(c if c.isalnum() or c in ['_', '-'] else '_' for c in identifier)
            cache_file = os.path.join(CACHE_DIR, f"{identifier}.pkl")
            
            # 1. Try Load
            if os.path.exists(cache_file):
                mtime = os.path.getmtime(cache_file)
                if (time.time() - mtime) < (ttl_hours * 3600):
                    print(f"[Cache] Loading {func.__name__} from {cache_file}...")
                    try:
                        with open(cache_file, 'rb') as f:
                            return pickle.load(f)
                    except Exception as e:
                        print(f"[Cache] Read failed: {e}")
            
            # 2. Fetch
            result = func(*args, **kwargs)
            
            # 3. Save (if valid)
            if isinstance(result, pd.DataFrame):
                 if not result.empty:
                    try:
                        with open(cache_file, 'wb') as f:
                            pickle.dump(result, f)
                        print(f"[Cache] Saved {func.__name__} to {cache_file}")
                    except Exception as e:
                         print(f"[Cache] Write failed: {e}")
            
            return result
        return wrapper
    return decorator

def retry(times=3, delay=2):
    """
    Retry decorator for robust API calls.
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            for i in range(times):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    print(f"[Warning] Attempt {i+1}/{times} failed for {func.__name__}: {e}")
                    time.sleep(delay)
            print(f"[Error] All {times} attempts failed for {func.__name__}.")
            return pd.DataFrame() # Return empty DF on failure
        return wrapper
    return decorator

def fetch_tencent_spot_data():
    """
    Backup Priority #1: Tencent (qtimg).
    Provides Price, PE (Dynamic), Volume Ratio.
    Mapping:
    ~3: Price
    ~32: Change%
    ~39: PE (Dynamic)
    ~49: Volume Ratio
    """
    print("Trying Tencent Finance Backup...")
    try:
        # 1. Get Code List
        stock_info = ak.stock_info_a_code_name()
        codes = stock_info['code'].tolist()
        
        # 2. Batch Request
        # Format: sh600519
        data_list = []
        batch_size = 60
        
        print(f"Fetching {len(codes)} stocks from Tencent in batches...")
        
        for i in range(0, len(codes), batch_size):
            batch = codes[i:i+batch_size]
            query_list = []
            for code in batch:
                prefix = 'sh' if code.startswith('6') else 'sz' if code.startswith('0') or code.startswith('3') else ''
                if prefix: query_list.append(f"{prefix}{code}")
            
            if not query_list: continue
            
            # Tencent API supports comma separated list
            url = f"http://qt.gtimg.cn/q={','.join(query_list)}"
            
            try:
                resp = requests.get(url, timeout=5)
                # Encoding is usually GBK
                content = resp.text
                
                # Parse
                lines = content.split(';')
                for line in lines:
                    line = line.strip()
                    if not line: continue
                    if '="' in line:
                        parts = line.split('="')
                        # v_sh600519
                        symbol_part = parts[0]
                        values = parts[1].strip('"').split('~')
                        
                        if len(values) > 50:
                            symbol = values[2] # 600519
                            name = values[1]
                            price = float(values[3])
                            change_pct = float(values[32])
                            
                            # Safe Parse PE (Index 39)
                            pe = values[39]
                            pe_ttm = float(pe) if pe and pe != '' else None
                            
                            # Safe Parse Vol Ratio (Index 49)
                            vr = values[49]
                            vol_ratio = float(vr) if vr and vr != '' else 0.0
                            
                            data_list.append({
                                'ä»£ç ': symbol,
                                'åç§°': name,
                                'æœ€æ–°ä»·': price,
                                'æ¶¨è·Œå¹…': change_pct,
                                'å¸‚ç›ˆç‡-åŠ¨æ€': pe_ttm,
                                'é‡æ¯”': vol_ratio
                            })
            except Exception as e:
                print(f"Tencent Batch Failed: {e}")
            
            time.sleep(0.1)
            
        return pd.DataFrame(data_list)
        
    except Exception as e:
        print(f"Tencent Backup Failed: {e}")
        return pd.DataFrame()

@with_cache(ttl_hours=8)
@retry(times=3, delay=5)
def fetch_spot_data():
    print("Fetching Spot Data...")
    try:
        # Primary: AkShare (Eastmoney)
        df = ak.stock_zh_a_spot_em()
        return df[['ä»£ç ', 'åç§°', 'æœ€æ–°ä»·', 'æ¶¨è·Œå¹…', 'å¸‚ç›ˆç‡-åŠ¨æ€', 'é‡æ¯”']].copy()
    except Exception as e:
        print(f"Primary Source Failed: {e}")
        # Secondary: Tencent (Better Data: PE + VolRatio)
        return fetch_tencent_spot_data()

@with_cache(ttl_hours=8)
@retry(times=3, delay=5)
def fetch_annual_eps(date="20241231"):
    print(f"Fetching Annual Report Data ({date})...")
    df = ak.stock_yjbb_em(date=date)
    # Need: Code, EPS ('æ¯è‚¡æ”¶ç›Š')
    if 'æ¯è‚¡æ”¶ç›Š' not in df.columns:
        # Fallback for older API versions or structure changes
        print("Warning: 'æ¯è‚¡æ”¶ç›Š' column not found directly. Columns:", df.columns)
        return pd.DataFrame()
    return df[['è‚¡ç¥¨ä»£ç ', 'æ¯è‚¡æ”¶ç›Š']].copy()

@with_cache(ttl_hours=8)
@retry(times=3, delay=5)
def fetch_growth_rate(date="20250930"):
    print(f"Fetching Growth Data ({date})...")
    df = ak.stock_yjbb_em(date=date)
    # Need: Code, Net Profit Growth ('å‡€åˆ©æ¶¦-åŒæ¯”å¢é•¿'), Industry ('æ‰€å¤„è¡Œä¸š')
    if 'å‡€åˆ©æ¶¦-åŒæ¯”å¢é•¿' not in df.columns:
         print("Warning: 'å‡€åˆ©æ¶¦-åŒæ¯”å¢é•¿' column not found. Columns:", df.columns)
         return pd.DataFrame()
    # Ensure Industry column exists
    # Ensure Industry column exists
    if 'æ‰€å¤„è¡Œä¸š' not in df.columns:
        print("Warning: 'æ‰€å¤„è¡Œä¸š' column not found.")
        # Create dummy if missing to avoid crash, but filtering will fail
        df['æ‰€å¤„è¡Œä¸š'] = 'Unknown'
        
    return df[['è‚¡ç¥¨ä»£ç ', 'å‡€åˆ©æ¶¦-åŒæ¯”å¢é•¿', 'æ‰€å¤„è¡Œä¸š']].copy()

@retry(times=3, delay=5)
def fetch_history(symbol, start_date, end_date):
    """
    Fetch daily history for a stock.
    """
    try:
        # ak.stock_zh_a_hist takes 6 digit code.
        df = ak.stock_zh_a_hist(symbol=symbol, period="daily", start_date=start_date, end_date=end_date, adjust="qfq")
        if df.empty: return pd.DataFrame()
        # Columns: æ—¥æœŸ, å¼€ç›˜, æ”¶ç›˜, æœ€é«˜, æœ€ä½, æˆäº¤é‡, æˆäº¤é¢...
        return df[['æ—¥æœŸ', 'æ”¶ç›˜', 'æˆäº¤é‡', 'æ¶¨è·Œå¹…']].copy()
    except Exception as e:
        print(f"Error fetching history for {symbol}: {e}")
        return pd.DataFrame()

def calculate_macd(df, fast=12, slow=26, signal=9):
    """
    Calculate MACD indicators.
    """
    if df.empty or len(df) < slow: return 0, 0, 0 # Not enough data
    
    # Calculate EMAs
    ema_fast = df['æ”¶ç›˜'].ewm(span=fast, adjust=False).mean()
    ema_slow = df['æ”¶ç›˜'].ewm(span=slow, adjust=False).mean()
    
    # Calculate DIF and DEA
    dif = ema_fast - ema_slow
    dea = dif.ewm(span=signal, adjust=False).mean()
    macd = (dif - dea) * 2
    
    # Return latest values
    return dif.iloc[-1], dea.iloc[-1], macd.iloc[-1]

def analyze_startup_phase(row):
    """
    Check if stock is in 'Initial Startup Phase' (High potential).
    Logic (Optimized):
    1. Volume Expanding (Ratio > 1.2).
    2. Price Stable (5-Day Chg < 20%).
    3. Technical: MACD Golden Cross or Bullish (DIF > DEA).
    Returns: (bool is_startup, str reason, float price_change)
    """
    end_date = datetime.datetime.now().strftime("%Y%m%d")
    start_date = (datetime.datetime.now() - datetime.timedelta(days=60)).strftime("%Y%m%d") # Need more history for MACD
    
    hist_df = fetch_history(row['Symbol'], start_date, end_date)
    if hist_df.empty or len(hist_df) < 26: # Need enough for Slow EMA
        return False, "Insufficient Data", 0.0
    
    # MACD Calculation
    dif, dea, macd = calculate_macd(hist_df)
    is_macd_bullish = (dif > dea) or (macd > 0)
    
    # Get last 5 trading days
    last_5 = hist_df.tail(5)
    
    # Volume Trend
    vol_recent = last_5['æˆäº¤é‡'].iloc[-2:].mean()
    vol_prev = last_5['æˆäº¤é‡'].iloc[:-2].mean()
    vol_trend_ratio = vol_recent / vol_prev if vol_prev > 0 else 0
    
    # Price Trend
    price_change = last_5['æ¶¨è·Œå¹…'].sum()
    
    # Logic:
    # 1. Volume expanding (Ratio > 1.2)
    # 2. Price Rising (Change > 0) but not Overheated (Change < 20)
    # 3. MACD Bullish
    is_startup = (vol_trend_ratio > 1.2) and (0 < price_change < 20) and is_macd_bullish
    
    macd_str = "Bullish" if is_macd_bullish else "Bearish"
    reason = f"Vol: {vol_trend_ratio:.2f}x, 5D: {price_change:.1f}%, MACD: {macd_str}"
    print(f"DEBUG: {row['Name']} - {reason}")
    return is_startup, reason, price_change

def analyze_potential_breakout(row):
    """
    Check if stock is in 'Pre-Breakout Consolidation' (Low Elevation, High Potential).
    Criteria:
    1. Consolidation: 5-Day Change between -3% and +5%. (Not flew yet)
    2. Trend Support: Price > MA20 (Bullish).
    3. Volume: Healthy (0.8 < Ratio < 2.5).
    4. Technical: MACD Bullish or Turning Up.
    
    Returns: (bool is_potential, str reason, float score_boost)
    """
    end_date = datetime.datetime.now().strftime("%Y%m%d")
    start_date = (datetime.datetime.now() - datetime.timedelta(days=60)).strftime("%Y%m%d")
    
    hist_df = fetch_history(row['Symbol'], start_date, end_date)
    if hist_df.empty or len(hist_df) < 26: 
        return False, "Insufficient Data", 0.0
        
    # 1. Calc MA20
    hist_df['MA20'] = hist_df['æ”¶ç›˜'].rolling(window=20).mean()
    current_price = hist_df['æ”¶ç›˜'].iloc[-1]
    ma20 = hist_df['MA20'].iloc[-1]
    
    # 2. MACD
    dif, dea, macd = calculate_macd(hist_df)
    is_macd_bullish = (dif > dea) or (macd > 0) or (dif > dif * 0.9) # Improving or Bullish
    
    # 3. 5-Day Change (Consolidation)
    last_5 = hist_df.tail(5)
    price_change_5d = last_5['æ¶¨è·Œå¹…'].sum()
    
    # 4. Filter Logic
    is_consolidating = (-3 <= price_change_5d <= 8) # Tight range
    is_above_support = (current_price >= ma20)
    
    vol_ratio = row.get('Volume_Ratio', 0)
    is_volume_healthy = (0.8 <= vol_ratio <= 3.0) # Not too cold, not too hot
    
    is_potential = is_consolidating and is_above_support and is_macd_bullish and is_volume_healthy
    
    reason = []
    if is_consolidating: reason.append(f"æ¨ªç›˜éœ‡è¡({price_change_5d:.1f}%)")
    if is_above_support: reason.append("MA20æ”¯æ’‘")
    if is_macd_bullish: reason.append("è¶‹åŠ¿å‘ä¸Š")
    
    score_boost = 0
    if is_potential:
        score_boost = 30 # Big boost for this pattern
        # Extra points for very tight consolidation
        if abs(price_change_5d) < 3: score_boost += 10
        # Extra points for perfect support test
        if 1.0 <= (current_price / ma20) <= 1.03: score_boost += 10
        
    return is_potential, " ".join(reason), score_boost

def fetch_and_analyze():
    # 1. Fetch all data
    spot_df = fetch_spot_data()
    if spot_df.empty:
        print("Critical: Failed to fetch spot data.")
        return None, pd.DataFrame()
        
    eps_df = fetch_annual_eps("20241231")
    # If 2024 Annual isn't out (it's Jan 2026), 2024 should be available. 
    # If not, we might need 2023. But let's assume 2024 is available or we use TTM logic strictly.
    # Actually, for "Static PE", we usually use last full year. In Jan 2026, 2024 Annual is the last full year.
    # Note: 2025 Annual report is NOT out in Jan 2026 (usually Apr 2026).
    # So Static PE should be based on 2024 Annual EPS? 
    # Or 2025 if available? Stocks usually release annual reports Jan-Apr.
    # To be safe, let's try 20241231. If empty, maybe try 20231231? 
    # Let's stick to the plan: fetch 2024.
    
    growth_df = fetch_growth_rate("20250930") # Q3 2025

    # 2. Rename columns for merging
    print(f"Spot DF Columns (Before Rename): {spot_df.columns.tolist()}")
    spot_df.rename(columns={'ä»£ç ': 'Symbol', 'åç§°': 'Name', 'æœ€æ–°ä»·': 'Price', 'æ¶¨è·Œå¹…': 'Change_Pct', 'å¸‚ç›ˆç‡-åŠ¨æ€': 'PE_TTM', 'é‡æ¯”': 'Volume_Ratio'}, inplace=True)
    print(f"Spot DF Columns (After Rename): {spot_df.columns.tolist()}")
    if not eps_df.empty:
        eps_df.rename(columns={'è‚¡ç¥¨ä»£ç ': 'Symbol', 'æ¯è‚¡æ”¶ç›Š': 'EPS'}, inplace=True)
    else: 
        eps_df = pd.DataFrame(columns=['Symbol', 'EPS'])

    if not growth_df.empty:
        growth_df.rename(columns={'è‚¡ç¥¨ä»£ç ': 'Symbol', 'å‡€åˆ©æ¶¦-åŒæ¯”å¢é•¿': 'Growth_Rate', 'æ‰€å¤„è¡Œä¸š': 'Industry'}, inplace=True)
    else:
        growth_df = pd.DataFrame(columns=['Symbol', 'Growth_Rate', 'Industry'])

    # 3. Merge
    print("Merging Data...")
    merged = pd.merge(spot_df, eps_df, on='Symbol', how='left')
    merged = pd.merge(merged, growth_df, on='Symbol', how='left')
    
    # 4. Cleanup types
    cols = ['Price', 'PE_TTM', 'EPS', 'Growth_Rate', 'Volume_Ratio']
    for col in cols:
        merged[col] = pd.to_numeric(merged[col], errors='coerce')
        
    # 5. Calculate Static PE and PEG
    merged['PE_Static'] = merged.apply(lambda row: row['Price'] / row['EPS'] if (pd.notnull(row['EPS']) and row['EPS'] > 0) else None, axis=1)
    merged['PEG'] = merged.apply(lambda row: row['PE_TTM'] / row['Growth_Rate'] if (pd.notnull(row['Growth_Rate']) and row['Growth_Rate'] > 0 and pd.notnull(row['PE_TTM'])) else None, axis=1)
    merged['PE_Improvement_Ratio'] = (merged['PE_Static'] - merged['PE_TTM']) / merged['PE_Static']

    # --- SECTOR RESONANCE ANALYSIS ---
    print("Calculating Sector Heat...")
    # Group by Industry and calculate mean price change
    sector_heat = merged.groupby('Industry')['Change_Pct'].mean().sort_values(ascending=False)
    # Identify Top 10 Hot Sectors
    top_10_sectors = sector_heat.head(10).index.tolist()
    print(f"Top 5 Hot Sectors: {top_10_sectors[:5]}")
    
    # Add Sector Score (Bonus)
    merged['Is_Hot_Sector'] = merged['Industry'].apply(lambda x: x in top_10_sectors)

    # 6. Advanced Filter & Rank
    print("Applying Advanced Filters (Hard Tech + Turnaround + Volume)...")
    
    # Hard Tech Industries (Whitelist)
    hard_tech_list = [
        'ç”µå­å…ƒä»¶', 'åŠå¯¼ä½“', 'å…‰å­¦å…‰ç”µå­', 'æ¶ˆè´¹ç”µå­', # Electronics
        'è®¡ç®—æœºè®¾å¤‡', 'è½¯ä»¶å¼€å‘', 'äº’è”ç½‘æœåŠ¡', # AI/Computer
        'é€šä¿¡è®¾å¤‡', 'é€šä¿¡æœåŠ¡', # Comm
        'èˆªå¤©èˆªç©º', 'èˆ¹èˆ¶åˆ¶é€ ', # Defense
        'å…‰ä¼è®¾å¤‡', 'é£ç”µè®¾å¤‡', 'ç”µæ± ', 'ç”µç½‘è®¾å¤‡', # Power/New Energy
        'ä¸“ç”¨è®¾å¤‡', 'é€šç”¨è®¾å¤‡', 'è‡ªåŠ¨åŒ–è®¾å¤‡', # Machinery
        'ç”Ÿç‰©åˆ¶å“', 'åŒ–å­¦åˆ¶è¯', 'åŒ»ç–—å™¨æ¢°' # Bio
    ]
    # Note: 'Industry' names from Eastmoney are usually specific. We might need partial match or broad categories.
    # The list above attempts to cover common Eastmoney/Shenwan L2 names. 
    # To be safe, we check if the industry *contains* key terms if exact match fails, or use a broader list.
    # Let's use a simpler keyword approach for robustness.
    tech_keywords = ['ç”µå­', 'åŠå¯¼ä½“', 'è®¡ç®—æœº', 'è½¯ä»¶', 'é€šä¿¡', 'èˆªå¤©', 'èˆªç©º', 'å…‰ä¼', 'é£ç”µ', 'ç”µæ± ', 'è®¾å¤‡', 'ç”Ÿç‰©', 'åˆ¶è¯', 'åŒ»ç–—']
    
    def is_hard_tech(ind):
        if not isinstance(ind, str): return False
        return any(k in ind for k in tech_keywords)

    # Check Data Integrity for Fallback
    has_pe = merged['PE_TTM'].count() > 0
    has_vol = (merged['Volume_Ratio'].max() > 0) if not merged['Volume_Ratio'].empty else False
    
    if has_pe and has_vol:
        print("Primary Data Integrity Confirmed.")
        filtered = merged[
            (merged['PE_TTM'] > 0) &
            (merged['PEG'] > 0) & 
            (merged['PEG'] < 1) &
            (merged['PE_TTM'] < merged['PE_Static']) &
            
            # Turnaround Criteria:
            (merged['Growth_Rate'] > 50) & # High Growth
            (merged['PE_Improvement_Ratio'] > 0.3) & # Significant Valuation Repair
            
            # Volume Criteria:
            (merged['Volume_Ratio'] > 1.5) # Abnormal Volume
        ].copy()
        sort_col = 'Volume_Ratio'
    else:
        # Fallback Mode (Sina)
        print("!! Fallback Mode Detected (Missing PE/Volume). Relaxing filters. !!")
        # Filter by Growth and Industry only (Price > 0, Growth > 30)
        filtered = merged[
            (merged['Price'] > 0) &
            (merged['Growth_Rate'] > 30) # Relaxed Growth
        ].copy()
        sort_col = 'Change_Pct' # Sort by Gain since Volume is 0
    
    # Apply Industry Filter
    filtered = filtered[filtered['Industry'].apply(is_hard_tech)]
    
    # Sort by Volume Ratio (descending) -> "Hot" Turnaround
    # Sort
    filtered.sort_values(by=sort_col, ascending=False, inplace=True)
    
    # Limit to top 3 (User Request)
    top_3 = filtered.head(3).copy()
    
    # 7. Analyze Trend (Startup Phase & Pre-Breakout)
    # Optimization: Run analysis on ALL filtered stocks (not just top 20) to find hidden gems.
    # But for performance (API cost), looking at top 50 matches is safer.
    print(f"Matched Criteria: {len(filtered)}. Selecting Top Candidates...")
    
    candidates = filtered.head(50).copy()
    candidates['Is_Startup'] = False
    candidates['Is_Potential'] = False # Pre-Breakout
    candidates['Reason'] = ""
    candidates['Price_Change_5D'] = 0.0
    candidates['Final_Score'] = 0.0
    
    for idx, row in candidates.iterrows():
        # Check Startup (Existing Logic)
        is_startup, s_reason, pchg = analyze_startup_phase(row)
        
        # Check Pre-Breakout (New Logic)
        is_potential, p_reason, p_score = analyze_potential_breakout(row)
        
        candidates.at[idx, 'Is_Startup'] = is_startup
        candidates.at[idx, 'Is_Potential'] = is_potential
        candidates.at[idx, 'Price_Change_5D'] = pchg
        
        # Scoring Logic (Hybrid)
        # Base: Volume Ratio * 10
        vol = row.get('Volume_Ratio', 0)
        if pd.isna(vol): vol = 0
        score = vol * 10
        
        if row.get('Is_Hot_Sector', False): score += 20
        if is_startup: score += 10
        if is_potential: score += p_score # Boost for Potential Breakout (+30~50)
        
        # Penalize if already rose too much (User feedback)
        if pchg > 20: score -= 20 
        
        candidates.at[idx, 'Final_Score'] = score
        
        # Generate Remark
        remarks = []
        if is_potential: remarks.append("âœ¨è“„åŠ¿å¾…å‘") # Highlight this!
        elif is_startup: remarks.append("ğŸš€æŠ€æœ¯å¯åŠ¨")
        
        if row.get('Is_Hot_Sector', False): remarks.append("ğŸ”¥çƒ­é—¨æ¿å—")
        if row.get('PE_Improvement_Ratio', 0) > 0.5: remarks.append("ä¼°å€¼ä¿®å¤")
        
        candidates.at[idx, 'Remark'] = " ".join(remarks) if remarks else "æˆé•¿ä½ä¼°"
        
        print(f"Analyzed {row['Name']}: Score={score:.1f} (Potential={is_potential}, Startup={is_startup})")

    # Sort by Final Modified Score
    candidates.sort_values(by='Final_Score', ascending=False, inplace=True)
    
    # Top 3
    top_3 = candidates.head(3).copy()

    # Select Golden Stock (Winner)
    if not top_3.empty:
        golden_row = top_3.iloc[0]
        
        # Build Reasoning
        is_pot = golden_row.get('Is_Potential', False)
        
        sector_str = "ğŸ”¥ å¤„äºä»Šæ—¥å¼ºåŠ¿é¢†æ¶¨æ¿å—" if golden_row.get('Is_Hot_Sector', False) else f"æ‰€å±{golden_row['Industry']}æ¿å—"
        
        if is_pot:
            trend_type = "âœ¨ æ½œä¼é‡‘è‚¡ (è“„åŠ¿å¾…å‘)"
            tech_str = "è‚¡ä»·å›è¸©MA20æ”¯æ’‘ï¼Œè¿‘æœŸç¼©é‡æ¨ªç›˜æ•´ç†ï¼ŒæŠ€æœ¯æŒ‡æ ‡(MACD)é‡‘å‰å‘ä¸Šï¼Œå…·å¤‡æé«˜çˆ†å‘æ½œåŠ›"
        elif golden_row['Is_Startup']:
            trend_type = "ğŸš€ å¯åŠ¨é‡‘è‚¡ (å³ä¾§äº¤æ˜“)"
            tech_str = "æŠ€æœ¯é¢å‘ˆç°å®Œç¾å¯åŠ¨å½¢æ€ (MACDé‡‘å‰/å¤šå¤´)ï¼Œèµ„é‡‘æŠ¢ç­¹æ˜æ˜¾"
        else:
            trend_type = "ğŸ’ ä»·å€¼é‡‘è‚¡ (æˆé•¿é©±åŠ¨)"
            tech_str = "èµ„é‡‘å…³æ³¨åº¦æé«˜ï¼ŒåŸºæœ¬é¢å¼ºåŠ²"
        
        vol_val = golden_row.get('Volume_Ratio', 0)
        vol_str = f"é‡æ¯” {vol_val:.2f}å€" if vol_val > 0 else f"æ¶¨å¹… {golden_row.get('Change_Pct', 0):.2f}%"
        
        pe_ttm_val = golden_row.get('PE_TTM', 'N/A')
        pe_str = f"åŠ¨æ€PE({pe_ttm_val})"
        
        logic_str = f"ç»¼åˆè¯„åˆ†ç¬¬ä¸€ã€‚{trend_type}ã€‚{vol_str}ï¼Œå‡€åˆ©å¢é•¿ {golden_row['Growth_Rate']:.0f}%ã€‚"
        advantage_str = f"ä¸šç»©å¤„äºåŠ é€Ÿé‡Šæ”¾æœŸï¼Œ{pe_str}ï¼Œ{sector_str}ã€‚"
        why_str = f"{tech_str}ï¼Œç›¸æ¯”è¿½é«˜æ›´å…·å®‰å…¨è¾¹é™…ã€‚"
        
        golden_stock = {
            'Symbol': golden_row['Symbol'],
            'Name': golden_row['Name'],
            'Price': golden_row['Price'],
            'Industry': golden_row['Industry'],
            'Logic': logic_str,
            'Advantage': advantage_str,
            'Why': why_str
        }
    else:
        golden_stock = None
    
    print("-" * 30)
    print(f"Selected Top: {len(top_3)}")
    print("-" * 30)
    
    if not top_3.empty:
        # Save to CSV for inspection
        cols_to_save = ['Symbol', 'Name', 'Industry', 'Price', 'PE_Static', 'PE_TTM', 'Growth_Rate', 'PEG', 'Volume_Ratio', 'Is_Startup', 'Is_Potential', 'Remark', 'Final_Score']
        top_3[cols_to_save].to_csv("stock_recommendations.csv", index=False)
        return golden_stock, top_3
    else:
        return None, pd.DataFrame()

if __name__ == "__main__":
    fetch_and_analyze()
